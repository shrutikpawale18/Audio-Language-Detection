{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25f4111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\daksh\\anaconda3\\lib\\site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\daksh\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31c4692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import IPython.display as ipd \n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten\n",
    "from keras.layers import Dropout, Input, Activation\n",
    "from keras.optimizers import Nadam, SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ad0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fb_and_mfcc(signal, sample_rate):\n",
    "\n",
    "    # Pre-Emphasis\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(\n",
    "        signal[0],\n",
    "        signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    # Framing\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "\n",
    "    # Convert from seconds to samples\n",
    "    frame_length, frame_step = (\n",
    "        frame_size * sample_rate,\n",
    "        frame_stride * sample_rate)\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "\n",
    "    # Make sure that we have at least 1 frame\n",
    "    num_frames = int(\n",
    "        np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "\n",
    "    # Pad Signal to make sure that all frames have equal\n",
    "    # number of samples without truncating any samples\n",
    "    # from the original signal\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "\n",
    "    indices = (\n",
    "        np.tile(np.arange(0, frame_length), (num_frames, 1)) +\n",
    "        np.tile(\n",
    "            np.arange(0, num_frames * frame_step, frame_step),\n",
    "            (frame_length, 1)\n",
    "        ).T\n",
    "    )\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "\n",
    "    # Window\n",
    "    frames *= np.hamming(frame_length)\n",
    "\n",
    "    # Fourier-Transform and Power Spectrum\n",
    "    NFFT = 512\n",
    "\n",
    "    # Magnitude of the FFT\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "\n",
    "    # Power Spectrum\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "\n",
    "    # Filter Banks\n",
    "    nfilt = 40\n",
    "\n",
    "    low_freq_mel = 0\n",
    "\n",
    "    # Convert Hz to Mel\n",
    "    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))\n",
    "\n",
    "    # Equally spaced in Mel scale\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "\n",
    "    # Convert Mel to Hz\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "\n",
    "    # Numerical Stability\n",
    "    filter_banks = np.where(\n",
    "        filter_banks == 0,\n",
    "        np.finfo(float).eps,\n",
    "        filter_banks)\n",
    "\n",
    "    # dB\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "    \n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_data\\singal_df_21001_21500.csv\n",
      "00:08:41.67\n",
      "ex_data\\singal_df_21501_22000.csv\n",
      "00:08:26.79\n",
      "ex_data\\singal_df_22001_22500.csv\n",
      "00:08:13.20\n",
      "ex_data\\singal_df_22501_23000.csv\n",
      "00:08:06.56\n",
      "ex_data\\singal_df_23001_23500.csv\n",
      "00:08:16.20\n",
      "ex_data\\singal_df_23501_24000.csv\n",
      "00:08:09.25\n",
      "ex_data\\singal_df_24001_24500.csv\n",
      "00:08:15.24\n",
      "ex_data\\singal_df_24501_25000.csv\n",
      "00:08:10.71\n",
      "ex_data\\singal_df_25001_25500.csv\n",
      "00:08:10.91\n",
      "ex_data\\singal_df_25501_26000.csv\n",
      "00:08:17.70\n",
      "ex_data\\singal_df_26001_26500.csv\n",
      "00:08:19.73\n",
      "ex_data\\singal_df_26501_27000.csv\n",
      "00:08:12.39\n",
      "ex_data\\singal_df_27001_27500.csv\n",
      "00:08:06.03\n",
      "ex_data\\singal_df_27501_28000.csv\n",
      "00:08:17.65\n",
      "ex_data\\singal_df_28001_28500.csv\n",
      "00:08:13.45\n",
      "ex_data\\singal_df_28501_29000.csv\n",
      "00:08:11.87\n",
      "ex_data\\singal_df_29001_29500.csv\n",
      "00:08:13.86\n",
      "ex_data\\singal_df_29501_30000.csv\n",
      "00:08:14.90\n",
      "ex_data\\singal_df_30001_30500.csv\n",
      "00:08:10.13\n",
      "ex_data\\singal_df_30501_31000.csv\n",
      "00:08:14.70\n",
      "ex_data\\singal_df_31001_31500.csv\n",
      "00:08:15.47\n",
      "ex_data\\singal_df_31501_32000.csv\n",
      "00:08:17.41\n",
      "ex_data\\singal_df_32001_32500.csv\n",
      "00:08:14.19\n",
      "ex_data\\singal_df_32501_33000.csv\n",
      "00:08:13.60\n",
      "ex_data\\singal_df_33001_33500.csv\n",
      "00:08:13.26\n",
      "ex_data\\singal_df_33501_34000.csv\n",
      "00:08:20.66\n",
      "ex_data\\singal_df_34001_34500.csv\n",
      "00:08:18.77\n",
      "ex_data\\singal_df_34501_35000.csv\n",
      "00:08:16.10\n",
      "ex_data\\singal_df_35001_35500.csv\n",
      "00:08:14.41\n",
      "ex_data\\singal_df_35501_36000.csv\n",
      "00:08:14.71\n",
      "ex_data\\singal_df_36001_36500.csv\n",
      "00:08:17.00\n",
      "ex_data\\singal_df_36501_37000.csv\n",
      "00:08:11.91\n",
      "ex_data\\singal_df_37001_37500.csv\n",
      "00:08:16.47\n",
      "ex_data\\singal_df_37501_38000.csv\n",
      "00:08:13.47\n"
     ]
    }
   ],
   "source": [
    "list1=list(pd.DataFrame(os.listdir('train'))[0].apply(lambda x: \"train\\\\\"+x).values)\n",
    "\n",
    "for i in range(42,round(len(list1)/500)):\n",
    "    sigdf=pd.DataFrame()\n",
    "    start = time.time()\n",
    "    for j in range(500):\n",
    "        signal, sample_rate = sf.read(list1[i*500+j])\n",
    "        df=pd.DataFrame(signal.reshape(-1, len(signal)))\n",
    "        df['sample_rate']=sample_rate\n",
    "        df['filename']=list1[i*500+j]\n",
    "        sigdf=pd.concat([sigdf,df])\n",
    "    sigdf=sigdf.reset_index(drop=True)\n",
    "    filehandler = open(r'ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+500)+'.pkl',\"wb\")\n",
    "    pickle.dump(sigdf,filehandler)\n",
    "    filehandler.close()\n",
    "    hours, rem = divmod(time.time()-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print('ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+500)+'.csv')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab543e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>lang</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>fragment</th>\n",
       "      <th>edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>0809fd0642232f8c85b0b3d545dc2b5a</td>\n",
       "      <td>fragment1</td>\n",
       "      <td>flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>0809fd0642232f8c85b0b3d545dc2b5a</td>\n",
       "      <td>fragment1</td>\n",
       "      <td>noise1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>0809fd0642232f8c85b0b3d545dc2b5a</td>\n",
       "      <td>fragment1</td>\n",
       "      <td>noise10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>0809fd0642232f8c85b0b3d545dc2b5a</td>\n",
       "      <td>fragment1</td>\n",
       "      <td>noise11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>0809fd0642232f8c85b0b3d545dc2b5a</td>\n",
       "      <td>fragment1</td>\n",
       "      <td>noise12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73075</th>\n",
       "      <td>train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>f7d959494477e5e7e33d4666f15311c9</td>\n",
       "      <td>fragment9</td>\n",
       "      <td>speed4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73076</th>\n",
       "      <td>train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>f7d959494477e5e7e33d4666f15311c9</td>\n",
       "      <td>fragment9</td>\n",
       "      <td>speed5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73077</th>\n",
       "      <td>train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>f7d959494477e5e7e33d4666f15311c9</td>\n",
       "      <td>fragment9</td>\n",
       "      <td>speed6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73078</th>\n",
       "      <td>train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>f7d959494477e5e7e33d4666f15311c9</td>\n",
       "      <td>fragment9</td>\n",
       "      <td>speed7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73079</th>\n",
       "      <td>train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>f7d959494477e5e7e33d4666f15311c9</td>\n",
       "      <td>fragment9</td>\n",
       "      <td>speed8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73080 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0 lang gender  \\\n",
       "0      train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...   de      f   \n",
       "1      train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...   de      f   \n",
       "2      train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...   de      f   \n",
       "3      train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...   de      f   \n",
       "4      train\\de_f_0809fd0642232f8c85b0b3d545dc2b5a.fr...   de      f   \n",
       "...                                                  ...  ...    ...   \n",
       "73075  train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...   es      m   \n",
       "73076  train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...   es      m   \n",
       "73077  train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...   es      m   \n",
       "73078  train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...   es      m   \n",
       "73079  train\\es_m_f7d959494477e5e7e33d4666f15311c9.fr...   es      m   \n",
       "\n",
       "                                user_id   fragment     edit  \n",
       "0      0809fd0642232f8c85b0b3d545dc2b5a  fragment1     flac  \n",
       "1      0809fd0642232f8c85b0b3d545dc2b5a  fragment1   noise1  \n",
       "2      0809fd0642232f8c85b0b3d545dc2b5a  fragment1  noise10  \n",
       "3      0809fd0642232f8c85b0b3d545dc2b5a  fragment1  noise11  \n",
       "4      0809fd0642232f8c85b0b3d545dc2b5a  fragment1  noise12  \n",
       "...                                 ...        ...      ...  \n",
       "73075  f7d959494477e5e7e33d4666f15311c9  fragment9   speed4  \n",
       "73076  f7d959494477e5e7e33d4666f15311c9  fragment9   speed5  \n",
       "73077  f7d959494477e5e7e33d4666f15311c9  fragment9   speed6  \n",
       "73078  f7d959494477e5e7e33d4666f15311c9  fragment9   speed7  \n",
       "73079  f7d959494477e5e7e33d4666f15311c9  fragment9   speed8  \n",
       "\n",
       "[73080 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=list(pd.DataFrame(os.listdir('train'))[0].apply(lambda x: \"train\\\\\"+x).values)\n",
    "df1=pd.DataFrame(list1)\n",
    "df1['lang']=df1[0].apply(lambda x: x.split('_')[0][-2:])\n",
    "df1['gender']=df1[0].apply(lambda x: x.split('_')[1])\n",
    "df1['user_id']=df1[0].apply(lambda x: x.split('_')[-1].split('.')[0])\n",
    "df1['fragment']=df1[0].apply(lambda x: x.split('_')[-1].split('.')[1])\n",
    "df1['edit']=df1[0].apply(lambda x: x.split('_')[-1].split('.')[2])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd58644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "de    28\n",
       "en    28\n",
       "es    28\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupby('lang')['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c74f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de    5366\n",
      "es    5365\n",
      "en    5346\n",
      "Name: lang, dtype: int64\n",
      "en    19014\n",
      "es    18995\n",
      "de    18994\n",
      "Name: lang, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df1,df1['lang'],stratify = df1[['lang','gender','user_id','edit']],test_size = 0.78,random_state = 420)\n",
    "print(X_train['lang'].value_counts())\n",
    "print(X_test['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bddc5411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_data\\singal_df_1_500.csv\n",
      "00:08:12.23\n",
      "ex_data\\singal_df_501_1000.csv\n",
      "00:08:19.91\n",
      "ex_data\\singal_df_1001_1500.csv\n",
      "00:08:11.50\n",
      "ex_data\\singal_df_1501_2000.csv\n",
      "00:08:17.88\n",
      "ex_data\\singal_df_2001_2500.csv\n",
      "00:08:06.83\n",
      "ex_data\\singal_df_2501_3000.csv\n",
      "00:08:10.14\n",
      "ex_data\\singal_df_3001_3500.csv\n",
      "00:08:14.32\n",
      "ex_data\\singal_df_3501_4000.csv\n",
      "00:08:16.95\n",
      "ex_data\\singal_df_4001_4500.csv\n",
      "00:08:27.09\n",
      "ex_data\\singal_df_4501_5000.csv\n",
      "00:08:20.96\n",
      "ex_data\\singal_df_5001_5500.csv\n",
      "00:08:24.45\n",
      "ex_data\\singal_df_5501_6000.csv\n",
      "00:08:28.48\n",
      "ex_data\\singal_df_6001_6500.csv\n",
      "00:08:30.14\n",
      "ex_data\\singal_df_6501_7000.csv\n",
      "00:08:23.40\n",
      "ex_data\\singal_df_7001_7500.csv\n",
      "00:08:31.53\n",
      "ex_data\\singal_df_7501_8000.csv\n",
      "00:08:29.93\n",
      "ex_data\\singal_df_8001_8500.csv\n",
      "00:08:27.33\n",
      "ex_data\\singal_df_8501_9000.csv\n",
      "00:08:30.27\n",
      "ex_data\\singal_df_9001_9500.csv\n",
      "00:08:23.53\n",
      "ex_data\\singal_df_9501_10000.csv\n",
      "00:08:25.53\n",
      "ex_data\\singal_df_10001_10500.csv\n",
      "00:08:20.80\n",
      "ex_data\\singal_df_10501_11000.csv\n",
      "00:08:26.70\n",
      "ex_data\\singal_df_11001_11500.csv\n",
      "00:08:24.16\n",
      "ex_data\\singal_df_11501_12000.csv\n",
      "00:08:25.71\n",
      "ex_data\\singal_df_12001_12500.csv\n",
      "00:08:25.09\n",
      "ex_data\\singal_df_12501_13000.csv\n",
      "00:08:32.75\n",
      "ex_data\\singal_df_13001_13500.csv\n",
      "00:08:31.94\n",
      "ex_data\\singal_df_13501_14000.csv\n",
      "00:08:27.40\n",
      "ex_data\\singal_df_14001_14500.csv\n",
      "00:08:30.09\n",
      "ex_data\\singal_df_14501_15000.csv\n",
      "00:08:36.66\n",
      "ex_data\\singal_df_15001_15500.csv\n",
      "00:08:34.50\n",
      "ex_data\\singal_df_15501_16000.csv\n",
      "00:08:09.26\n"
     ]
    }
   ],
   "source": [
    "list1=X_train[0].values\n",
    "\n",
    "for i in range(round(len(list1)/500)):\n",
    "    sigdf=pd.DataFrame()\n",
    "    start = time.time()\n",
    "    for j in range(500):\n",
    "        signal, sample_rate = sf.read(list1[i*500+j])\n",
    "        df=pd.DataFrame(signal.reshape(-1, len(signal)))\n",
    "        df['sample_rate']=sample_rate\n",
    "        df['filename']=list1[i*500+j]\n",
    "        sigdf=pd.concat([sigdf,df])\n",
    "    sigdf=sigdf.reset_index(drop=True)\n",
    "    filehandler = open(r'ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+j+1)+'.pkl',\"wb\")\n",
    "    pickle.dump(sigdf,filehandler)\n",
    "    filehandler.close()\n",
    "    hours, rem = divmod(time.time()-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print('ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+j+1)+'.csv')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b7aebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>220498</th>\n",
       "      <th>220499</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>filename</th>\n",
       "      <th>220500</th>\n",
       "      <th>lang</th>\n",
       "      <th>gender</th>\n",
       "      <th>user_id</th>\n",
       "      <th>fragment</th>\n",
       "      <th>edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.020905</td>\n",
       "      <td>-0.031860</td>\n",
       "      <td>-0.028931</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\es_f_1d27c6d589eeff17973ffd0b7a77a70a.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>f</td>\n",
       "      <td>1d27c6d589eeff17973ffd0b7a77a70a</td>\n",
       "      <td>fragment5</td>\n",
       "      <td>speed5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007965</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>-0.005829</td>\n",
       "      <td>-0.019653</td>\n",
       "      <td>-0.028564</td>\n",
       "      <td>-0.035706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018646</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\es_f_53b555eab2b3baada380f7d3ede20b20.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>f</td>\n",
       "      <td>53b555eab2b3baada380f7d3ede20b20</td>\n",
       "      <td>fragment14</td>\n",
       "      <td>pitch4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.070190</td>\n",
       "      <td>-0.066132</td>\n",
       "      <td>-0.063599</td>\n",
       "      <td>-0.062103</td>\n",
       "      <td>-0.059601</td>\n",
       "      <td>-0.055115</td>\n",
       "      <td>-0.050598</td>\n",
       "      <td>-0.046295</td>\n",
       "      <td>-0.043060</td>\n",
       "      <td>-0.038849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\de_f_d94712992f41e3d8d21f22274b3d8fd9.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>d94712992f41e3d8d21f22274b3d8fd9</td>\n",
       "      <td>fragment24</td>\n",
       "      <td>noise6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005951</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>-0.009888</td>\n",
       "      <td>-0.012848</td>\n",
       "      <td>-0.014374</td>\n",
       "      <td>-0.015961</td>\n",
       "      <td>-0.013062</td>\n",
       "      <td>-0.013824</td>\n",
       "      <td>-0.015961</td>\n",
       "      <td>-0.020050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006683</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\en_f_10134f409d9b7b0b95fed6e025febcad.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>f</td>\n",
       "      <td>10134f409d9b7b0b95fed6e025febcad</td>\n",
       "      <td>fragment25</td>\n",
       "      <td>noise7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038666</td>\n",
       "      <td>-0.038940</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\es_m_b8e0e6f56f02e6f8f79cc360958e5982.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>b8e0e6f56f02e6f8f79cc360958e5982</td>\n",
       "      <td>fragment8</td>\n",
       "      <td>noise4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.016266</td>\n",
       "      <td>-0.013611</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.035248</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019928</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\es_m_d5b91a4ffb1ead826b7968ec19cbfa1c.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>m</td>\n",
       "      <td>d5b91a4ffb1ead826b7968ec19cbfa1c</td>\n",
       "      <td>fragment3</td>\n",
       "      <td>noise10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\de_m_fc6bd6bb9d66a89bb8d8a8a7efa23e6b.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>m</td>\n",
       "      <td>fc6bd6bb9d66a89bb8d8a8a7efa23e6b</td>\n",
       "      <td>fragment4</td>\n",
       "      <td>noise6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>0.022522</td>\n",
       "      <td>0.032166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>-0.018768</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\de_m_d22535879801cc9c4452d9ed9de5bf61.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>m</td>\n",
       "      <td>d22535879801cc9c4452d9ed9de5bf61</td>\n",
       "      <td>fragment20</td>\n",
       "      <td>speed4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.002380</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>-0.004486</td>\n",
       "      <td>-0.004395</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.006195</td>\n",
       "      <td>-0.012115</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>-0.005341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\de_f_2825fa225d6ca4800f0cf0504b76ca65.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>f</td>\n",
       "      <td>2825fa225d6ca4800f0cf0504b76ca65</td>\n",
       "      <td>fragment11</td>\n",
       "      <td>pitch6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.006256</td>\n",
       "      <td>-0.006653</td>\n",
       "      <td>-0.027435</td>\n",
       "      <td>-0.017883</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.021820</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.009186</td>\n",
       "      <td>-0.001740</td>\n",
       "      <td>-0.008087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004730</td>\n",
       "      <td>-0.002106</td>\n",
       "      <td>22050</td>\n",
       "      <td>train\\es_f_bf4285930fa46f2052e5bdbc37a8a4df.fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>f</td>\n",
       "      <td>bf4285930fa46f2052e5bdbc37a8a4df</td>\n",
       "      <td>fragment21</td>\n",
       "      <td>speed2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 220508 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.020905 -0.031860 -0.028931 -0.020203 -0.002075  0.010193  0.013611   \n",
       "1   -0.007965 -0.007202  0.003601  0.010895  0.014069  0.005890 -0.005829   \n",
       "2   -0.070190 -0.066132 -0.063599 -0.062103 -0.059601 -0.055115 -0.050598   \n",
       "3   -0.005951 -0.011993 -0.009888 -0.012848 -0.014374 -0.015961 -0.013062   \n",
       "4    0.001556  0.001404  0.001617  0.002106  0.002625  0.002869  0.001709   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "495 -0.016266 -0.013611  0.000397  0.020569  0.032318  0.035248  0.035309   \n",
       "496  0.000122  0.001282  0.002045  0.002472  0.002960  0.003479  0.003418   \n",
       "497  0.012054  0.015717  0.015717  0.018982  0.018433  0.018677  0.016052   \n",
       "498 -0.002380  0.003052  0.003204 -0.004486 -0.004395  0.001587  0.006195   \n",
       "499  0.006256 -0.006653 -0.027435 -0.017883  0.003571  0.021820  0.022003   \n",
       "\n",
       "            7         8         9  ...    220498    220499  sample_rate  \\\n",
       "0    0.004120 -0.009247 -0.012665  ...  0.000000  0.000000        22050   \n",
       "1   -0.019653 -0.028564 -0.035706  ...  0.018646  0.011688        22050   \n",
       "2   -0.046295 -0.043060 -0.038849  ...  0.015717  0.017517        22050   \n",
       "3   -0.013824 -0.015961 -0.020050  ... -0.006683 -0.006561        22050   \n",
       "4    0.000946  0.001740  0.002380  ... -0.038666 -0.038940        22050   \n",
       "..        ...       ...       ...  ...       ...       ...          ...   \n",
       "495  0.038361  0.045197  0.046844  ...  0.019928  0.016479        22050   \n",
       "496  0.002960  0.002747  0.002960  ...  0.013397  0.016937        22050   \n",
       "497  0.018707  0.022522  0.032166  ... -0.018799 -0.018768        22050   \n",
       "498 -0.012115 -0.008118 -0.005341  ...  0.000000  0.000000        22050   \n",
       "499  0.009186 -0.001740 -0.008087  ... -0.004730 -0.002106        22050   \n",
       "\n",
       "                                              filename  220500  lang  gender  \\\n",
       "0    train\\es_f_1d27c6d589eeff17973ffd0b7a77a70a.fr...     NaN    es       f   \n",
       "1    train\\es_f_53b555eab2b3baada380f7d3ede20b20.fr...     NaN    es       f   \n",
       "2    train\\de_f_d94712992f41e3d8d21f22274b3d8fd9.fr...     NaN    de       f   \n",
       "3    train\\en_f_10134f409d9b7b0b95fed6e025febcad.fr...     NaN    en       f   \n",
       "4    train\\es_m_b8e0e6f56f02e6f8f79cc360958e5982.fr...     NaN    es       m   \n",
       "..                                                 ...     ...   ...     ...   \n",
       "495  train\\es_m_d5b91a4ffb1ead826b7968ec19cbfa1c.fr...     NaN    es       m   \n",
       "496  train\\de_m_fc6bd6bb9d66a89bb8d8a8a7efa23e6b.fr...     NaN    de       m   \n",
       "497  train\\de_m_d22535879801cc9c4452d9ed9de5bf61.fr...     NaN    de       m   \n",
       "498  train\\de_f_2825fa225d6ca4800f0cf0504b76ca65.fr...     NaN    de       f   \n",
       "499  train\\es_f_bf4285930fa46f2052e5bdbc37a8a4df.fr...     NaN    es       f   \n",
       "\n",
       "                              user_id    fragment     edit  \n",
       "0    1d27c6d589eeff17973ffd0b7a77a70a   fragment5   speed5  \n",
       "1    53b555eab2b3baada380f7d3ede20b20  fragment14   pitch4  \n",
       "2    d94712992f41e3d8d21f22274b3d8fd9  fragment24   noise6  \n",
       "3    10134f409d9b7b0b95fed6e025febcad  fragment25   noise7  \n",
       "4    b8e0e6f56f02e6f8f79cc360958e5982   fragment8   noise4  \n",
       "..                                ...         ...      ...  \n",
       "495  d5b91a4ffb1ead826b7968ec19cbfa1c   fragment3  noise10  \n",
       "496  fc6bd6bb9d66a89bb8d8a8a7efa23e6b   fragment4   noise6  \n",
       "497  d22535879801cc9c4452d9ed9de5bf61  fragment20   speed4  \n",
       "498  2825fa225d6ca4800f0cf0504b76ca65  fragment11   pitch6  \n",
       "499  bf4285930fa46f2052e5bdbc37a8a4df  fragment21   speed2  \n",
       "\n",
       "[500 rows x 220508 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "file = open(r'ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+500)+'.pkl','rb')\n",
    "sigdf = pickle.load(file)\n",
    "file.close()\n",
    "sigdf['lang']=sigdf['filename'].apply(lambda x: x.split('_')[0][-2:])\n",
    "sigdf['gender']=sigdf['filename'].apply(lambda x: x.split('_')[1])\n",
    "sigdf['user_id']=sigdf['filename'].apply(lambda x: x.split('_')[-1].split('.')[0])\n",
    "sigdf['fragment']=sigdf['filename'].apply(lambda x: x.split('_')[-1].split('.')[1])\n",
    "sigdf['edit']=sigdf['filename'].apply(lambda x: x.split('_')[-1].split('.')[2])\n",
    "sigdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1b21897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:40:24.57\n"
     ]
    }
   ],
   "source": [
    "list1=X_train[0].values\n",
    "language_dummies=pd.DataFrame()\n",
    "MFCC_array = []\n",
    "sc = StandardScaler()\n",
    "start=time.time()\n",
    "for i in range(round(len(list1)/500)):\n",
    "    file = open(r'ex_data\\singal_df_'+str(i*500+1)+'_'+str(i*500+500)+'.pkl','rb')\n",
    "    sigdf = pickle.load(file)\n",
    "    singal_values=np.array(sigdf.iloc[:,:220500])\n",
    "    language_dummies = pd.concat([language_dummies,pd.get_dummies(sigdf['filename'].apply(lambda x: x.split('_')[0][-2:]))])\n",
    "    for i in range(0,len(singal_values)):\n",
    "        MFCC = generate_fb_and_mfcc(singal_values[i], sigdf['sample_rate'][i])\n",
    "        MFCC_sc = sc.fit_transform(MFCC)\n",
    "        MFCC_array.append(MFCC_sc)\n",
    "    \n",
    "MFCC_array = np.array(MFCC_array)\n",
    "filehandler = open(r'ex_data\\MFCC_arrays.pkl',\"wb\")\n",
    "pickle.dump(MFCC_array,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(r'ex_data\\language_dummies.pkl',\"wb\")\n",
    "pickle.dump(language_dummies,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "hours, rem = divmod(time.time()-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "959d518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_MFCC,X_test_MFCC,y_train_MFCC,y_test_MFCC = train_test_split(MFCC_array,language_dummies,stratify = language_dummies,test_size = 0.20,random_state = 0)\n",
    "X_train_MFCC = X_train_MFCC.reshape(-1,1000,40,1)\n",
    "X_test_MFCC = X_test_MFCC.reshape(-1,1000,40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3782fb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800, 1000, 40, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_MFCC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b28d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1334s - loss: 0.7079 - accuracy: 0.7370 - val_loss: 0.2464 - val_accuracy: 0.9125 - lr: 0.0014 - 1334s/epoch - 3s/step\n",
      "Epoch 2/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1351s - loss: 0.1522 - accuracy: 0.9444 - val_loss: 0.7528 - val_accuracy: 0.7681 - lr: 0.0013 - 1351s/epoch - 3s/step\n",
      "Epoch 3/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1339s - loss: 0.0914 - accuracy: 0.9676 - val_loss: 0.1603 - val_accuracy: 0.9384 - lr: 0.0012 - 1339s/epoch - 3s/step\n",
      "Epoch 4/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1331s - loss: 0.0574 - accuracy: 0.9803 - val_loss: 0.1582 - val_accuracy: 0.9419 - lr: 0.0010 - 1331s/epoch - 3s/step\n",
      "Epoch 5/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1324s - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.1417 - val_accuracy: 0.9541 - lr: 9.3297e-04 - 1324s/epoch - 3s/step\n",
      "Epoch 6/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1325s - loss: 0.0278 - accuracy: 0.9903 - val_loss: 0.1026 - val_accuracy: 0.9669 - lr: 8.3968e-04 - 1325s/epoch - 3s/step\n",
      "Epoch 7/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1328s - loss: 0.0299 - accuracy: 0.9904 - val_loss: 0.4905 - val_accuracy: 0.8641 - lr: 7.5571e-04 - 1328s/epoch - 3s/step\n",
      "Epoch 8/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1335s - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.6055 - val_accuracy: 0.8244 - lr: 6.8014e-04 - 1335s/epoch - 3s/step\n",
      "Epoch 9/9\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "400/400 - 1328s - loss: 0.0246 - accuracy: 0.9912 - val_loss: 0.0249 - val_accuracy: 0.9919 - lr: 6.1212e-04 - 1328s/epoch - 3s/step\n",
      "03:20:01.29\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "input_shape = (1000,40,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(7, 7), activation='relu', padding='valid', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "model.add(Conv2D(64,(5,5), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "model.add(Conv2D(128,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "model.add(Conv2D(256,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "model.add(Conv2D(512,(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=2, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "adam = Adam()\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.00158\n",
    "    drop = 0.9\n",
    "    epochs_drop = 1\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\n",
    "                'model.h5',\n",
    "                monitor='val_acc',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                mode='max'\n",
    "                )\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "model.fit(X_train_MFCC,\n",
    "          y_train_MFCC,\n",
    "          epochs=9,\n",
    "          callbacks=[checkpoint, lrate],\n",
    "          verbose=2,\n",
    "          validation_data=(X_test_MFCC, y_test_MFCC),\n",
    "          batch_size=32)\n",
    "\n",
    "hours, rem = divmod(time.time()-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bee0ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\batch_normalization\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_4\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_5\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_6\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_4\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...layers\\max_pooling2d_2\n",
      "......vars\n",
      "...layers\\max_pooling2d_3\n",
      "......vars\n",
      "...layers\\max_pooling2d_4\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-05-01 02:56:15         9709\n",
      "metadata.json                                  2023-05-01 02:56:15           64\n",
      "variables.h5                                   2023-05-01 02:56:15     40573896\n"
     ]
    }
   ],
   "source": [
    "filehandler = open(r'ex_data\\model.pkl',\"wb\")\n",
    "pickle.dump(model,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd90e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 326s 815ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train_MFCC)\n",
    "y_train1 = []\n",
    "label={0:'de',1:'en',2:'es'}\n",
    "for i in range(0,len(y_train_MFCC)):\n",
    "    argmax = label[np.argmax(y_train_MFCC.iloc[i,:])]\n",
    "    y_train1.append(argmax)\n",
    "y_pred1 = []\n",
    "for i in range(0,len(y_train_MFCC)):\n",
    "    argmax = label[np.argmax(y_pred[i,:])]\n",
    "    y_pred1.append(argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e26c722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.18316166e-07, 9.99996185e-01, 3.67341750e-06],\n",
       "       [2.07718131e-12, 9.99999881e-01, 6.15733313e-08],\n",
       "       [8.20629372e-13, 1.08308784e-09, 1.00000000e+00],\n",
       "       ...,\n",
       "       [8.38450420e-10, 9.99999881e-01, 9.89855238e-08],\n",
       "       [2.79180984e-10, 7.18146822e-08, 9.99999881e-01],\n",
       "       [7.83233523e-01, 2.13708773e-01, 3.05774994e-03]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0fc83e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4262,    4,    8],\n",
       "       [   0, 4256,    0],\n",
       "       [   3,    0, 4267]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train1,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "baf09929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          de       1.00      1.00      1.00      4274\n",
      "          en       1.00      1.00      1.00      4256\n",
      "          es       1.00      1.00      1.00      4270\n",
      "\n",
      "    accuracy                           1.00     12800\n",
      "   macro avg       1.00      1.00      1.00     12800\n",
      "weighted avg       1.00      1.00      1.00     12800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train1,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed559958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 80s 796ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test_MFCC)\n",
    "y_test1 = []\n",
    "label={0:'de',1:'en',2:'es'}\n",
    "for i in range(0,len(y_test_MFCC)):\n",
    "    argmax = label[np.argmax(y_test_MFCC.iloc[i,:])]\n",
    "    y_test1.append(argmax)\n",
    "y_pred2 = []\n",
    "for i in range(0,len(y_test_MFCC)):\n",
    "    argmax = label[np.argmax(y_pred_test[i,:])]\n",
    "    y_pred2.append(argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b62be5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1054,    5,    9],\n",
       "       [   5, 1055,    4],\n",
       "       [   2,    1, 1065]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test1,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f388ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          de       0.99      0.99      0.99      1068\n",
      "          en       0.99      0.99      0.99      1064\n",
      "          es       0.99      1.00      0.99      1068\n",
      "\n",
      "    accuracy                           0.99      3200\n",
      "   macro avg       0.99      0.99      0.99      3200\n",
      "weighted avg       0.99      0.99      0.99      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test1,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add samples of predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
